import streamlit as st
import re # å¯¼å…¥æ­£åˆ™åº“

# è®¾ç½®ç½‘é¡µé…ç½®
st.set_page_config(page_title="BibTeX ç¼©å†™ä¸æ¸…æ´—å·¥å…·", layout="centered")

st.title('BibTeX æ™ºèƒ½å¤„ç†å·¥å…·')
st.markdown("ä¸Šä¼  .bib æ–‡ä»¶ï¼Œè‡ªåŠ¨ç¼©å†™æœŸåˆŠåï¼Œå¹¶å¯é€‰æ‹©æ¸…ç†å†—ä½™çš„ DOIã€‚")

# --- 1. åŠ è½½ç¼©å†™æ•°æ®åº“ ---
@st.cache_data
def load_journal_list():
    journal_map = []
    try:
        try:
            with open('journal_list.txt', 'r', encoding='utf-8') as fr:
                lines = fr.readlines()
        except UnicodeDecodeError:
            with open('journal_list.txt', 'r', encoding='latin-1') as fr:
                lines = fr.readlines()
                
        for line in lines:
            if " = " in line:
                parts = line.strip().split(" = ")
                if len(parts) >= 2:
                    full = parts[0]
                    short = parts[1]
                    full_fmt = '{%s}' % full
                    short_fmt = '{%s}' % short
                    
                    if full != full.upper() and (' ' in full):
                        journal_map.append((full_fmt, short_fmt))
        return journal_map
    except FileNotFoundError:
        return []

# --- æ–°å¢åŠŸèƒ½: æ¸…æ´— DOI/URL å†²çª ---
def clean_doi_conflict(content):
    """
    é€»è¾‘ï¼š
    1. æŒ‰ '@' åˆ†å‰²æ¡ç›®
    2. å¯¹æ¯ä¸ªæ¡ç›®æ£€æŸ¥ï¼Œå¦‚æœåŒæ—¶å«æœ‰ 'url =' å’Œ 'doi =' (å¿½ç•¥å¤§å°å†™)
    3. åˆ é™¤ doi é‚£ä¸€è¡Œ
    """
    # ä¸ºäº†é¿å… split åƒæ‰åˆ†éš”ç¬¦ï¼Œæˆ‘ä»¬å…ˆåœ¨æ‰€æœ‰ @ å‰åŠ ä¸ªç‰¹æ®Šæ ‡è®°ï¼Œæˆ–è€…æ‰‹åŠ¨æ‹†åˆ†
    # ç®€å•çš„åšæ³•æ˜¯æŒ‰ \n@ æ‹†åˆ† (å‡è®¾BibTeXæ ¼å¼æ ‡å‡†)
    # ä¸ºäº†å¤„ç†æ–¹ä¾¿ï¼Œæˆ‘ä»¬å…ˆç»™å¼€å¤´è¡¥ä¸€ä¸ªæ¢è¡Œ
    content = "\n" + content
    entries = re.split(r'(\n@)', content) 
    
    cleaned_entries = []
    removed_count = 0
    
    # splitååˆ—è¡¨ä¼šæ˜¯ ['', '\n@', 'article{...', '\n@', 'book{...']
    # æˆ‘ä»¬éœ€è¦é‡ç»„
    buffer = ""
    
    for part in entries:
        if part == '\n@':
            buffer = part
        else:
            full_entry = buffer + part
            buffer = "" # é‡ç½®
            
            if not full_entry.strip(): 
                continue

            # --- æ ¸å¿ƒåˆ¤æ–­é€»è¾‘ ---
            # æ£€æŸ¥è¿™ä¸ªæ¡ç›®é‡Œæ˜¯å¦æœ‰ url å’Œ doi å­—æ®µ
            # ä½¿ç”¨ç®€å•çš„å­—ç¬¦ä¸²åŒ…å«æ£€æŸ¥ (è½¬å°å†™)
            lower_entry = full_entry.lower()
            has_url = 'url =' in lower_entry or 'url=' in lower_entry
            has_doi = 'doi =' in lower_entry or 'doi=' in lower_entry
            
            if has_url and has_doi:
                # ä½¿ç”¨æ­£åˆ™åˆ é™¤ doi è¡Œ
                # æ­£åˆ™è§£é‡Š: 
                # (?m) å¤šè¡Œæ¨¡å¼
                # ^\s*doi\s*= åŒ¹é…è¡Œé¦–(å…è®¸ç©ºæ ¼)çš„doi=
                # .* åŒ¹é…è¿™è¡Œå‰©ä¸‹çš„æ‰€æœ‰å†…å®¹
                # \n? åŒ¹é…å¯èƒ½å­˜åœ¨çš„æ¢è¡Œç¬¦
                full_entry, n = re.subn(r'(?m)^\s*doi\s*=.*(\r?\n)?', '', full_entry, flags=re.IGNORECASE)
                if n > 0:
                    removed_count += 1
            
            cleaned_entries.append(full_entry)
            
    return "".join(cleaned_entries).strip(), removed_count

# --- 2. ä¾§è¾¹æ é…ç½®åŒº ---
st.sidebar.header("åŠŸèƒ½è®¾ç½®")
enable_abbr = st.sidebar.checkbox("å¯ç”¨æœŸåˆŠç¼©å†™æ›¿æ¢", value=True)
enable_doi_clean = st.sidebar.checkbox("è‹¥åŒæ—¶å­˜åœ¨URLå’ŒDOIï¼Œåˆ é™¤DOI", value=True, help="å½“ä¸€ä¸ªæ–‡çŒ®æ¡ç›®åŒæ—¶åŒ…å« URL å’Œ DOI å­—æ®µæ—¶ï¼Œä¿ç•™ URLï¼Œåˆ é™¤ DOI è¡Œã€‚")

# --- 3. æ–‡ä»¶ä¸Šä¼ ç»„ä»¶ ---
uploaded_file = st.file_uploader("è¯·é€‰æ‹© reference.bib æ–‡ä»¶", type=['bib', 'txt'])

# --- 4. å¤„ç†é€»è¾‘ ---
if uploaded_file is not None:
    # è¯»å–å†…å®¹
    bib_content = uploaded_file.getvalue().decode("utf-8", errors='ignore')
    st.info(f"æ–‡ä»¶å·²è¯»å–ï¼Œæ­£åœ¨å¤„ç†...")
    
    processed_content = bib_content
    logs = []

    # === æ­¥éª¤ A: æ¸…æ´— DOI (å¦‚æœä½ å‹¾é€‰äº†çš„è¯) ===
    if enable_doi_clean:
        processed_content, doi_removed_num = clean_doi_conflict(processed_content)
        if doi_removed_num > 0:
            logs.append(f"ğŸ§¹ æ¸…ç†å†²çª: åˆ é™¤äº† {doi_removed_num} ä¸ªå†—ä½™çš„ DOI å­—æ®µ (å› ä¸ºè¯¥æ¡ç›®å·²æœ‰ URL)ã€‚")
        else:
            logs.append("ğŸ§¹ æ¸…ç†å†²çª: æœªå‘ç°åŒæ—¶å­˜åœ¨ DOI å’Œ URL çš„æ¡ç›®ï¼Œæ— éœ€åˆ é™¤ã€‚")

    # === æ­¥éª¤ B: ç¼©å†™æ›¿æ¢ (å¦‚æœä½ å‹¾é€‰äº†çš„è¯) ===
    if enable_abbr:
        replacements = load_journal_list()
        if not replacements:
            st.error("é”™è¯¯ï¼šæ‰¾ä¸åˆ° journal_list.txt æ–‡ä»¶ã€‚")
        else:
            abbr_count = 0
            # è¿›åº¦æ¡é€»è¾‘
            progress_bar = st.progress(0)
            status_text = st.empty()
            total_items = len(replacements)
            update_step = int(total_items / 100) 

            for i, (full, short) in enumerate(replacements):
                if full in processed_content:
                    count = processed_content.count(full)
                    processed_content = processed_content.replace(full, short)
                    logs.append(f"âœ… ç¼©å†™æ›¿æ¢: {full} -> {short} (å…± {count} å¤„)")
                    abbr_count += 1
                
                if i % update_step == 0:
                    progress_bar.progress(min(i / total_items, 1.0))
            
            progress_bar.progress(1.0)
            status_text.text("å¤„ç†å®Œæˆï¼")
            
            if abbr_count == 0:
                logs.append("â„¹ï¸ ç¼©å†™æ£€æŸ¥: æœªå‘ç°éœ€è¦ç¼©å†™çš„æœŸåˆŠåã€‚")

    # --- 5. ç»“æœå±•ç¤ºåŒºåŸŸ ---
    st.success("æ‰€æœ‰æ“ä½œæ‰§è¡Œå®Œæ¯•ï¼")

    with st.expander("æŸ¥çœ‹è¯¦ç»†å¤„ç†æ—¥å¿—"):
        for log in logs:
            st.text(log)

    # --- 6. ä¸‹è½½æŒ‰é’® ---
    original_name = uploaded_file.name
    new_name = f"processed_{original_name}"
    
    st.download_button(
        label="â¬‡ï¸ ä¸‹è½½å¤„ç†åçš„æ–‡ä»¶",
        data=processed_content,
        file_name=new_name,
        mime="text/plain"
    )